prompt:
produce the docker files with command docker compose up
---------------------------------------------------------------------------------------------------------------------------------

Dockerfile:

# Base image with CUDA support
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04

# Install system dependencies and Python
RUN apt-get update && apt-get install -y python3 python3-pip

# Install Python packages including PyTorch, Hugging Face, and Flask
RUN pip3 install torch transformers datasets flask

# Copy application code into the Docker container
COPY . /app

# Set the working directory
WORKDIR /app

# Default command to start the Flask server
CMD ["python3", "server.py"]







-----------------------------------------------------------------------------------------------------------------------------------------




Server.py:

from flask import Flask, request, jsonify
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Initialize Flask app
app = Flask(__name__)

# Load the model and tokenizer
model = GPT2LMHeadModel.from_pretrained("gpt2")  # Use GPT-2 as the base model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

@app.route("/generate", methods=["POST"])
def generate_text():
    # Get the input prompt from the JSON request
    input_prompt = request.json.get("prompt", "")

    # Tokenize the prompt and generate text
    inputs = tokenizer.encode(input_prompt, return_tensors="pt")
    outputs = model.generate(
        inputs,
        max_length=50,  # Maximum generated length
        num_beams=5,  # Beam search with 5 beams
        early_stopping=True,
        no_repeat_ngram_size=2,  # Avoid repeating n-grams
    )

    # Decode and return the generated text
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return jsonify({"generated_text": generated_text})

# Run the Flask server
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8080)  # Expose on port 8080


-------------------------------------------------------------------------------------------------------------------------------

Docker-compose.yml:




version: '3.8'

services:
  llm-service:
    build: .  # Build from the current directory
    image: my-llm-image  # Image name
    ports:
      - "8080:8080"  # Expose port 8080 for the Flask server
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]  # Ensure the container has access to GPU
    environment:
      CUDA_VISIBLE_DEVICES: "0"  # Specify which GPU(s) to use




---------------------------------------------------------------------------------------------------------------------------------

Running with Docker Compose
To build and run the LLM with Docker Compose, you need to follow these steps:

Build the Docker Image:
Open a terminal in the directory where you have the Dockerfile, server.py, and docker-compose.yml. Run the following command to build the Docker image:
  docker-compose build


Run the Docker Container:
Once the image is built, start the service using docker-compose up:
  docker-compose up
